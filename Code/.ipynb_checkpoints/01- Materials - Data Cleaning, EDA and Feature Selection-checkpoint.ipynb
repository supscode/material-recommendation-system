{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Recommendation System for a Fashion Supply Chain Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fabric Central is a digital sourcing platform that connects apparel brands and fabric suppliers in the most efficient, accessible and transparent marketplace. Suppliers can display their materials on the platforms and Brand users can easily find the materials from various suppliers in one place. \n",
    "\n",
    "Fabric Central has onboarded around 78 suppliers with around 200 materials so far and has around 358 brands registered brands. However it would now like to focus on increasing their sales and engage better with their users. \n",
    "\n",
    "We will build a recommendation system for Fabric Central which will recommend relevant materials to the customer at various touchpoints on the platform. By showing relevant materials and helping the users fnd the right matrials for their needs, we hope to increase customer engagement and conversion rate. \n",
    "\n",
    "We will build two types of recommendations, \n",
    "1. Content Based\n",
    "    This will recommend similar materials to the one the customer is viewing, this can be displayed on the material search page, possible under a section called \"You may also like...\" or \"More like these...\" \n",
    "2. Item based Collaborative Filtering\n",
    "    This will recommend materials depending on the gabric request history of this material by other users. This can be displayed on the order confirmation page, possibly under a section called \"Users who bought this also bought...\"\n",
    "3. Hybrid recommendations\n",
    "    The fabric request data is found to be very sparse and is not available for all materials. To tackle this cold start problem, we will also provide hybrid recomendations. Here the collborative filtering related recommendations (which need fabric request data) will be generated first, and will be supplemented by the content based materials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warnings because there are some warnings related to SettingWithCopyWarning \n",
    "# inspite of using the correct code (.loc[row_indexer,col_indexer] = value)\n",
    "\n",
    "warnings.simplefilter('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Read the Data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xae in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xae in position 10: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-ebdb6251edc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaterials_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../datasets/raw_data/materials_orig.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvariants_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../datasets/raw_data/variants_orig.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtier_prices_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../datasets/raw_data/tier_prices_orig.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msuppliers_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../datasets/raw_data/supplier_msk_orig.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfab_req_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../datasets/raw_data/fabric_requests_orig.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xae in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "materials_orig = pd.read_csv(\"../datasets/raw_data/materials_orig.csv\", encoding=\"ISO-8859-1\")\n",
    "variants_orig = pd.read_csv(\"../datasets/raw_data/variants_orig.csv\")\n",
    "tier_prices_orig = pd.read_csv(\"../datasets/raw_data/tier_prices_orig.csv\")\n",
    "suppliers_orig = pd.read_csv(\"../datasets/raw_data/supplier_msk_orig.csv\")\n",
    "fab_req_orig = pd.read_csv(\"../datasets/raw_data/fabric_requests_orig.csv\")\n",
    "users_orig = pd.read_csv(\"../datasets/raw_data/usrs_msk_orig.csv\", encoding=\"ISO-8859-1\")\n",
    "brands_orig = pd.read_csv(\"../datasets/raw_data/brands_msk_orig.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning for tables related to Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a. Materials Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "materials_orig.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the materials which are unpublished, archived or don't have a price as these cannot be recommended to any users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only published, non-archived and materials with non-zero price\n",
    "materials = materials_orig[(materials_orig['published'] == True) & \n",
    "                           (materials_orig['archived_at'].isnull()) &\n",
    "                           (materials_orig['price'] > 0)\n",
    "                          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.isnull().sum()[materials.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns, combine variables which were split into multiple columns due to conversion from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all values are null we will drop these columns. \n",
    "materials.drop(columns=['deleted_at', 'archived_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine columns which are split into variables which were split into multiple columns \n",
    "# Returns a new dataset with combined column and old columns dropped\n",
    "\n",
    "def combine_columns(col_name, dataset):\n",
    "    out_dataset = dataset.copy()\n",
    "    cols_to_combine = []\n",
    "    # Loop through all columns. \n",
    "    for col in out_dataset.columns:\n",
    "        \n",
    "        # If column name contains col_name followed by a \"/\" (usually followed by a number e.g. properties/0) , \n",
    "        # then add to the list of columns to be combined\n",
    "        if col.find(col_name + '/') != -1:\n",
    "            cols_to_combine.append(col)\n",
    "            out_dataset[col] = out_dataset[col].str.strip()\n",
    "\n",
    "    # Combine the listed columns into a column with col_name\n",
    "    out_dataset[col_name] = out_dataset.apply(lambda x: x[cols_to_combine].dropna().to_list(), axis=1)\n",
    "\n",
    "    # Drop old columns e.g. properties/0, properties/1 etc. \n",
    "    out_dataset.drop(columns=cols_to_combine, inplace=True)\n",
    "        \n",
    "    return out_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine values fro, properties/0, properties/1 etc. columns as a list into a \n",
    "# common column\n",
    "materials = combine_columns('properties', materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate properties with pipe separated values\n",
    "def split_pipe_properties(properties):\n",
    "    properties1 = properties.copy()\n",
    "    for prop in properties:\n",
    "        if(prop.find(\"|\") > 0):\n",
    "            lst = prop.split(\"|\")\n",
    "            [properties.append(lst_item) for lst_item in lst]\n",
    "            properties.remove(prop)\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some properties values are strings with multiple properties separated by a \"|\" e.g. \"Durable Water Repellent (DWR)|Downproof\"\n",
    "# These are separate properties so we separate them and add back to the list\n",
    "\n",
    "materials['properties'] = materials['properties'].apply(lambda x: split_pipe_properties(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine values fro, only_for_brands/0, only_for_brands/1 etc. columns as a list into a \n",
    "# common column\n",
    "\n",
    "materials = combine_columns('only_for_brands', materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine values fro, end_use/0, end_use/1 etc. columns as a list into a \n",
    "# common column\n",
    "\n",
    "materials = combine_columns('end_use', materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some end_use values are strings with multiple end uses separated by a \"|\" e.g. \"Casual Wear|Menswear|Performance\"\n",
    "# These are separate properties so we separate them and add back to the list\n",
    "\n",
    "materials['end_use'] = materials['end_use'].apply(lambda x: split_pipe_properties(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the right Data Types for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "materials.dtypes[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.dtypes[41:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all date-times to datetime datatypes\n",
    "materials['created_at'] = pd.to_datetime(materials['created_at'])\n",
    "materials['updated_at'] = pd.to_datetime(materials['updated_at'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.isnull().sum()[materials.isnull().sum() > 0][0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop materials with no factory locations\n",
    "materials = materials.loc[materials['factory_location'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  fabric_blend_one has only 6 rows with missing values, so we can drop these values\n",
    "materials = materials[materials['fabric_blend_one'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find records where fabric_blend_two which is null when fabric_blend_two_percent is not null set fabric_blend_two_percent to 0\n",
    "#print(materials.loc[(materials['fabric_blend_two_percent'].notnull()) & (materials['fabric_blend_two_percent'] != 0) & (materials['fabric_blend_two'].isnull()), ['id', 'fabric_blend_two', 'fabric_blend_two_percent']])\n",
    "materials.loc[(materials['fabric_blend_two'].isnull()), ['fabric_blend_two_percent']] = 0\n",
    "\n",
    "# find records where fabric_blend_three which is null when fabric_blend_two_percent is not null, set fabric_blend_two_percent to 0\n",
    "#print(materials.loc[(materials['fabric_blend_three_percent'].notnull()) & (materials['fabric_blend_three_percent'] != 0) & (materials['fabric_blend_three'].isnull()), ['id', 'fabric_blend_three', 'fabric_blend_three_percent']])\n",
    "materials.loc[(materials['fabric_blend_three'].isnull()), ['fabric_blend_three_percent']] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fabric_blend_two and fabric_blend_three are optional fields as not all fabrics will\n",
    "# have multiple blends. So we replace it with blank strings\n",
    "\n",
    "materials['fabric_blend_two'] = materials['fabric_blend_two'].fillna(\"blank_blend_1\")\n",
    "materials['fabric_blend_three'] = materials['fabric_blend_three'].fillna(\"blank_blend_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values of null_column by taking mean of null_column by groupby_column\n",
    "def impute_mean_by_column(dataset, null_column, groupby_column):\n",
    "    # Group by the group by column and calculate mean value for null_column for each group\n",
    "    grouped = dataset.groupby(groupby_column)\n",
    "    transformed = grouped[null_column].mean()\n",
    "    # Join the groups dataset with the materials dataset on the group_by column in order to relate \n",
    "    # the materials with the right mean value to substitute \n",
    "    dataset = dataset.merge(transformed,on=groupby_column, how=\"left\")\n",
    "\n",
    "    x_null_column = null_column + '_x'\n",
    "    y_null_column = null_column + '_y'\n",
    "\n",
    "    # Fill null values in the x_null_column (from the materials dataset) with the mean values in the \n",
    "    # y_null_column (from the groups dataset)\n",
    "    dataset[x_null_column] = dataset[x_null_column].fillna(dataset[y_null_column])\n",
    "    \n",
    "    # Drop the y_null_column (originally from the groups_dataset) and \n",
    "    # rename x_null_column to original column name (null_column)\n",
    "    dataset.drop([y_null_column],inplace=True,axis=1)\n",
    "    dataset.rename(columns={x_null_column:null_column},inplace=True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weight for leather materials, where the weight is null to 500\n",
    "materials[materials['weight'].isnull() & materials['fabric_type'].str.contains('Leather')]['price'] = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weight for rest of the materials where the weight is 0\n",
    "materials['weight'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values of weight by taking mean dispatch_time_mass_quantity_min, dispatch_time_mass_quantity_max by supplier_id\n",
    "\n",
    "materials = impute_mean_by_column(materials, 'dispatch_time_mass_quantity_min', 'supplier_id')\n",
    "materials = impute_mean_by_column(materials, 'dispatch_time_mass_quantity_max', 'supplier_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values of weight by taking mean dispatch_time_mass_quantity_min by dispatch_time_sample\n",
    "\n",
    "materials = impute_mean_by_column(materials, 'dispatch_time_mass_quantity_min', 'dispatch_time_sample')\n",
    "materials = impute_mean_by_column(materials, 'dispatch_time_mass_quantity_max', 'dispatch_time_sample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values of null_column by taking mean of null_column by groupby_column\n",
    "def impute_mode_by_column(dataset, null_column, groupby_column):\n",
    "    # Group by the group by column and calculate mean value for null_column for each group\n",
    "    grouped = dataset.groupby(groupby_column)\n",
    "    transformed = grouped[null_column].agg(pd.Series.mode)\n",
    "    #transformed[transformed.str.len == 0, 'supplier_id'] = \"\"\n",
    "    \n",
    "    # Join the groups dataset with the materials dataset on the group_by column in order to relate \n",
    "    # the materials with the right mean value to substitute \n",
    "    dataset = dataset.merge(transformed,on=groupby_column, how=\"left\")\n",
    "\n",
    "    x_null_column = null_column + '_x'\n",
    "    y_null_column = null_column + '_y'\n",
    "\n",
    "    # Fill null values in the x_null_column (from the materials dataset) with the mean values in the \n",
    "    # y_null_column (from the groups dataset)\n",
    "    dataset[x_null_column] = dataset[x_null_column].fillna(dataset[y_null_column])\n",
    "    \n",
    "    # Drop the y_null_column (originally from the groups_dataset) and \n",
    "    # rename x_null_column to original column name (null_column)\n",
    "    dataset.drop([y_null_column],inplace=True,axis=1)\n",
    "    dataset.rename(columns={x_null_column:null_column},inplace=True)\n",
    "    \n",
    "    # Set empty lists to null (mode returns an empty list when there are no values found in the group)\n",
    "    dataset.loc[dataset[null_column].str.len() == 0, [null_column]] = np.nan    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute dispatch_time_sample by finding the mode (most occurring value for the given supplier)\n",
    "materials = impute_mode_by_column(materials, 'dispatch_time_sample', 'supplier_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute dispatch_time_sample by finding the mode (most occurring value for the given factory location)\n",
    "materials = impute_mode_by_column(materials, 'dispatch_time_sample', 'factory_location')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the rest of the null values with \"blank_display_sample\" text\n",
    "materials['dispatch_time_sample'].fillna(\"blank_display_sample\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the null values of width by taking mean width by fabric_blend_one\n",
    "\n",
    "materials = impute_mean_by_column(materials, 'width', 'fabric_blend_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the remaining null values of width by taking mean width by fabric_weave\n",
    "\n",
    "materials = impute_mean_by_column(materials, 'width', 'fabric_weave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the null values of width_unit by taking mode width by fabric_blend_one\n",
    "\n",
    "materials = impute_mode_by_column(materials, 'width_unit', 'fabric_blend_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We replace null fabric weaves with blank\n",
    "materials['fabric_weave'].fillna(\"blank_weave\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 1 null weight unit found so we update it to the most common weight unit \n",
    "materials.loc[materials['weight_unit'].isnull(), 'weight_unit'] = materials.loc[materials['weight_unit'].notnull(), 'weight_unit'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.isnull().sum()[materials.isnull().sum() > 0][0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the null values of desicription as \"blank_description\"\n",
    "\n",
    "materials['description'].fillna(\"blank_description\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the null values of desicription as \"blank_finishing\"\n",
    "\n",
    "materials['finishing'].fillna(\"blank_finishing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For columns with too many nulls, and not relevant we drop them from materials dataset\n",
    "# Keep only relevant columns in the material dataset\n",
    "\n",
    "materials.drop(columns=['construction', 'origin', 'colour', 'tsa_stamps', 'crowd_source_available', 'moq_details'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Supplier Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this dataset to get information about suppliers to see if there can be relevant information which can be used to recomemnd materials to users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppliers_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null fabric_type, specialty, end_uses, fibers. \n",
    "# These values are specific to suppliers and cannot be imputed as per the domain experts. \n",
    "# It may not be possible to use the supplier data for recommendations for this reason\n",
    "# However, for now we will still go ahead with it.\n",
    "\n",
    "suppliers = suppliers_orig[['id', 'name', 'fabric_type', 'specialty', 'end_uses', 'fibers']]\n",
    "suppliers['fabric_type'].fillna('blank_fabrictype', inplace=True)\n",
    "suppliers['specialty'].fillna('blank_specialty', inplace=True)\n",
    "suppliers['end_uses'].fillna('blank_end_uses', inplace=True)\n",
    "suppliers['fibers'].fillna('blank_fibers', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppliers.rename(columns={'name': 'supp_name', 'id': 'supplier_id', 'fabric_type': 'supp_fabric_type', 'specialty': 'supp_specialty', 'end_uses': 'supp_end_uses', 'fibers': 'supp_fibers'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are many null values in the suppliers dataset features, we will not be able to use it for recommendations. However we will still merge it with materials dataset, for future use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Material Tier Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset stores tiered pricing for bulk orders. We will examine it to see if there is relevant information to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_prices_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_prices_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the created_at, updated_at fields as they are not relevant to the analysis\n",
    "tier_prices = tier_prices_orig[['id', 'material_id', 'starts_from', 'price']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'price' and 'id' columns so there is no conflict while merging with the materials dataset\n",
    "# Rename rest of the columns to align with the materials dataset\n",
    "\n",
    "tier_prices.rename(columns={'starts_from': 'tiered_price_starts_from','price':'tiered_price', 'id':'tier_price_id', 'material_id':'id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tier_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Material Variants Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset stores data about colour variants of the materials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variants_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_orig.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_orig.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop deleted_at column as all values are null\n",
    "# Drop created_at, updated_at, image columns as they are not relevant for our analysis\n",
    "# leftover column is also dropped as it is a legacy field, no longer in use\n",
    "variants = variants_orig[['id', 'colour_name', 'pantone', 'hex', 'material_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only those rows for which either colour name, pantone or hex values are not null\n",
    "variants = variants[(variants['colour_name'].notnull()) | (variants['pantone'].notnull()) | (variants['hex'].notnull())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename id and material_id columns to align the column names with the materials dataset\n",
    "variants.rename(columns={'id': 'variant_id', 'colour_name':'var_colour', 'pantone':'var_pantone', 'hex':'var_hex'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Merge Materials, Supplier datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials = materials.merge(suppliers, on='supplier_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Merge Material, Tier_prices and Variants datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the relevant fields from the tier_prices table with materials table\n",
    "materials_tiered_prices = materials.merge(tier_prices[['id','tiered_price_starts_from','tiered_price']], on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_tiered_prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the relevant fields from the variants dataset with materials dataset\n",
    "materials_merged = materials_tiered_prices.merge(variants[['material_id','var_colour','var_pantone', 'var_hex']], left_on='id', right_on='material_id', how='left')\n",
    "materials_merged.drop(columns=['material_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values in colour_name, pantone and hex with blanks\n",
    "materials_merged['var_colour'].fillna(\"blank_colour\", inplace=True)\n",
    "materials_merged['var_pantone'].fillna(\"blank_pantone\", inplace=True)\n",
    "materials_merged['var_hex'].fillna(\"blank_hex\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_merged.loc[materials_merged['tiered_price_starts_from'].isnull(), ['tiered_price_starts_from', 'tiered_price', 'price', 'minimal_order_quantity']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_merged[\"var_colour\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute fields that are null after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the tiered price to same as sample price and update tiered_price_starts_from to moq\n",
    "materials_merged.loc[materials_merged['tiered_price'].isnull(), ['tiered_price']] = materials_merged['price']\n",
    "materials_merged.loc[materials_merged['tiered_price_starts_from'].isnull(), ['tiered_price_starts_from']] = materials_merged['minimal_order_quantity']\n",
    "\n",
    "# Impute the tiered price to same as sample price and update tiered_price_starts_from to moq\n",
    "materials_tiered_prices.loc[materials_tiered_prices['tiered_price'].isnull(), ['tiered_price']] = materials_tiered_prices['price']\n",
    "materials_tiered_prices.loc[materials_tiered_prices['tiered_price_starts_from'].isnull(), ['tiered_price_starts_from']] = materials_tiered_prices['minimal_order_quantity']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging we oberve that most (over 1000) tier price and variant colours are blank, so we will not focus on these tables. \n",
    "\n",
    "We will focus our EDA on the materials table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EDA for tables related to Content Based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the countplot for column, sorted by descending order of counts \n",
    "def sorted_count_plot(dataset, column, blank_value=\"\", title=\"\", x_label=\"\", y_label=\"\"):\n",
    "    plt.figure(figsize=(12,14))\n",
    "    \n",
    "    # If blank_value is passed then exclude blank values and print the count\n",
    "    if blank_value != \"\":\n",
    "        print(f\"Blank values: {len(dataset[dataset[column] == blank_value])}\")\n",
    "        dataset = dataset[dataset[column] != blank_value]\n",
    "\n",
    "    sort_order = dataset.groupby(column)[\"id\"].count().sort_values(ascending=False).index.values\n",
    "    hp = sns.countplot(data=dataset, y=column, order=sort_order);\n",
    "\n",
    "    if(title == \"\"):\n",
    "        hp.set_title(f\"Distribution of materials by {column}\", fontsize=22);\n",
    "        hp.set_xlabel(\"Count\", fontsize=20);\n",
    "        hp.set_ylabel(column, fontsize=20);\n",
    "    else:\n",
    "        hp.set_title(title, fontsize=22);\n",
    "        hp.set_xlabel(x_label, fontsize=20);\n",
    "        hp.set_ylabel(y_label, fontsize=20);\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution for fabric_weave\n",
    "\n",
    "cp = sorted_count_plot(materials, \"fabric_weave\", title=\"Distribution of materials by Fabric Weave\", y_label=\"Fabric Weave\", x_label=\"Count of materials\")\n",
    "cp.set_yticklabels(cp.get_ymajorticklabels(), fontsize = 13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum fabrics have blank fabric_weave (688), followed by plain (280). However, this is an important feature to identify the right fabrics, so we may still use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution for fabric_blend_one\n",
    "\n",
    "cp = sorted_count_plot(materials, \"fabric_blend_one\", title=\"Distribution of materials by Fabric Blend One\", y_label=\"Fabric Blend One\", x_label=\"Count of materials\")\n",
    "cp.set_yticklabels(cp.get_ymajorticklabels(), fontsize = 13);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important field for content based recommendations, as per domain experts. Also has very few blank valyues which were dropped. We will include these in the content based recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution for supplier_id\n",
    "\n",
    "sorted_count_plot(materials, \"supplier_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials['supplier_id'].value_counts().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the materials belong to 40 unique suppliers. This is an important field for content based recommendations, as per domain experts. Also has very no blank values. We will include these in the content based recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the histogram for fabric_type\n",
    "cp = sorted_count_plot(materials, \"fabric_type\", title=\"Distribution of materials by Fabric Type\", y_label=\"Fabric Type\", x_label=\"Count of materials\")\n",
    "cp.set_yticklabels(cp.get_ymajorticklabels(), fontsize = 22);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a broad classification but an important one, as per domain experts and will help to avoid incorrect recommendations due to end_use. We will include these in the content based recommendation model. No blank values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for minimal_order_quantity\n",
    "plt.figure(figsize=(12,14))\n",
    "hp = sns.histplot(data=materials, x='minimal_order_quantity')\n",
    "plt.xticks(rotation=45);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifies minimal order quantity for orders. It's not the very relevant for recommendations and most materials have zero minimal order quantity, hence this field will not be useful for recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the histogram for price\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure(figsize=(12,14))\n",
    "cp = sns.histplot(data=materials[materials['price']!= 0], x='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials[(materials['price'] >  200)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price will be an important field for recomemnndations. Most materials have prices under 50\\\\$ per meter square. There is a set of materials containing Pirarucu (fish skin) from a supplier (52) which are around 250\\\\$ per meter square,  hence we see a spike at 250\\\\$ in the histogram. It is observed that some suppliers (52, 54) have high end fabrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the histogram for price\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure(figsize=(12,14))\n",
    "cp = sns.histplot(data=materials[materials['weight']!= 0], x='weight')\n",
    "\n",
    "cp.set_title(f\"Distribution of materials by weight\", fontsize=22);\n",
    "cp.set_xlabel(\"Weight\", fontsize=20);\n",
    "cp.set_ylabel(\"Count of materials\", fontsize=20);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight is a n important factor when predicting recommendations as designers often look for specific weight of fabrics when ordering. Most fabrics have non zero weights, making it a good factor for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Content Based recommendations we find that the materials dataset has some useful features which will be used for recommendations such as weight, price, fabric_blend_one, fabric_type etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data cleaning of tables related to Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Fabric Requests Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fabric request dataset includes fabric requests made by brand users. Fabric requests are the first step of an order. Only once the supplier confirms availability the user can place an order. However the fabric requests are stilla good indicator of user's interest in the fabric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_req_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_req_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_req_orig.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all date-times to datetime datatypes\n",
    "fab_req_orig['requested_at'] = pd.to_datetime(fab_req_orig['requested_at'])\n",
    "fab_req_orig['closed_at'] = pd.to_datetime(fab_req_orig['closed_at'])\n",
    "fab_req_orig['created_at'] = pd.to_datetime(fab_req_orig['created_at'])\n",
    "fab_req_orig['updated_at'] = pd.to_datetime(fab_req_orig['updated_at'])\n",
    "fab_req_orig['accepted_at'] = pd.to_datetime(fab_req_orig['accepted_at'])\n",
    "fab_req_orig['deleted_at'] = pd.to_datetime(fab_req_orig['deleted_at'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_req_orig.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are interested only in the information about which user has requested fabrics, we drop the other fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests = fab_req_orig.drop(columns=[\"requested_at\",\"closed_at\", \"delay_days\", \n",
    "                                          \"delay_note\", \"project_id\", \"created_at\",\n",
    "                                         \"updated_at\",\"order_id\", \"price\",  \"price_unit\",\n",
    "                                          \"surcharge\", \"accepted_at\", \"deleted_at\", \n",
    "                                          \"origin_quantity\", \"variant_id\", \"currency\",\n",
    "                                          \"bought_price\"\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Users Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains information about users of the FC system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column names to replace spaces with \"_\", and change to lower case\n",
    "def fix_col_name (col_name):\n",
    "    col_name = col_name.replace(\" \", \"_\")\n",
    "    col_name = col_name.lower()\n",
    "    return col_name\n",
    "\n",
    "users = users_orig.rename(columns = {orig_name: fix_col_name(orig_name) for orig_name in users_orig.columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need this dataset to filter out demo users. So we only keep the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users[[\"id\", \"email\", \"name\", \"activation_state\", \"role\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"id\" to \"user_id\" to avoid ambiguity after merging with the fabric_requests dataset\n",
    "users = users.rename(columns={\"email\":\"user_email\", \"id\":\"user_id\", \"name\":\"user_name\", \"activation_state\":\"user_activation_state\", \"role\":\"user_role\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Merge Fabric Request and Users datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests = fab_requests.merge(users, how=\"left\", on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d. Merge Fabric Request and Materials datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the fabric request and materials datasets to get material names, and to remove fabric requests for any deleted materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests = fab_requests.merge(materials[[\"id\", \"name\"]], how=\"inner\", left_on=\"material_id\", right_on=\"id\", suffixes=[None, \"_y\"])\n",
    "fab_requests.drop(columns=[\"id_y\"], inplace=True)\n",
    "fab_requests.rename(columns={\"name\":\"material_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop demo user ids from fabric requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the demo email ids, as they have requested on behalf of other offline users and this can skew the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all fabric requests with email ids which contain techstyle or takalar\n",
    "fab_requests = fab_requests[(fab_requests[\"user_email\"].str.contains(\"techstyle\") == False) | (fab_requests[\"user_email\"].str.contains(\"takalar\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests.groupby(by=\"material_name\").count()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. EDA of tables related to Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of fabric requests by type\n",
    "cp = sns.countplot(data=fab_requests, x='type')\n",
    "cp.set_title(\"Distribution of fabric requests by Type\")\n",
    "cp.set_xlabel(\"Request Type\")\n",
    "cp.set_ylabel(\"Count of Fabric Requests\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of fabric requests, swatch and sample. Swatch is a small piece of fabric used only checking the type of fabric.  Many more swatch requests are observed. Swatches are free, so we will need to give more weightage to sample requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of fabric requests by status\n",
    "cp = sns.countplot(data=fab_requests, x='status')\n",
    "cp.set_title(\"Distribution of fabric requests by Status\")\n",
    "cp.set_xlabel(\"Request Status\")\n",
    "cp.set_ylabel(\"Count of Fabric Requests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of fabric requests by status, type\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "cp = sns.countplot(data=fab_requests, y=\"status\", hue=\"type\", order=fab_requests.status.value_counts().index)\n",
    "cp.set_title(\"Distribution of fabric requests by status, Request Type\")\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no cancelled status included in the data we will include all fabric requests for purposes of recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fab_requests.user_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distribution of fabric requests by Materials, Request type\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "cp = sns.countplot(data=fab_requests, y=\"material_name\", hue=\"type\", order=fab_requests.material_name.value_counts().iloc[:50].index)\n",
    "cp.set_title(\"Distribution of fabric requests by Materials, Request Type (top 50 out of 349)\", fontsize=20)\n",
    "cp.set_xlabel(\"Count\", fontsize=16)\n",
    "cp.set_ylabel(\"Material Name\", fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize='x-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of fabric requests by user id, request type\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "cp = sns.countplot(data=fab_requests, y=\"user_id\", hue=\"type\", order=fab_requests.user_id.value_counts().iloc[:50].index)\n",
    "cp.set_title(\"Distribution of fabric requests by User Id, Request Type (top 50 out of 72) \", fontsize=16)\n",
    "cp.set_xlabel(\"Count of Fabric Requests\", fontsize=14)\n",
    "cp.set_ylabel(\"User Id\", fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize='x-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that user_id 284 has requested many more swatches (100) compared to actual samples (under 5). This may not be a serious user. So we remove this user from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fab_requests = fab_requests[fab_requests[\"user_id\"] != 284]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Datasets to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials.to_csv('../datasets/clean_data/materials_clean.csv', index=False)\n",
    "variants.to_csv('../datasets/clean_data/variants_clean.csv', index=False)\n",
    "tier_prices.to_csv('../datasets/clean_data/tiered_price_clean.csv', index=False)\n",
    "materials_merged.to_csv('../datasets/clean_data/materials_merged.csv', index=False)\n",
    "fab_requests.to_csv('../datasets/clean_data/fabric_requests_clean.csv', index=False)\n",
    "materials_tiered_prices.to_csv('../datasets/clean_data/materials_tiered_prices.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
